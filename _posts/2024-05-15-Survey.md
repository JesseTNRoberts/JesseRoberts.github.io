---
title: 'Survey of Foundation Model Pre-Training Tasks'
date: 2024-05-15
permalink: /posts/2024/05/15/Survey/
tags:
  - Idea
  - Manuscript
---

Foundation models require appropriate data, architectures, and pre-training tasks. The former two considerations are typical in long-standing machine learning research. However, the need for appropriate pre-training tasks is relatively new. While novel foundation models require appropriate tasks, model architects may find important inspiration in task innovations from successful and unsuccessful foundation model development efforts. While not all tasks are appropriate for every architecture or dataset, the goal of this survey is to provide a reference from which foundation model architects may draw task inspiration and intuition for the creation of new models.

## Language Pre-Training Tasks

next sentence
masked token
next token prediction


## Image Pre-Training

Image masking

### Image with Text Pre-Training

CLIP - association of caption with image
FLIP - association of caption with image with simultaneous image patch masking


## Video Pre-Training

Masked frame prediction


## High level perspective

Successful pre-training tasks require models to exploit semantically related information to succeed. Further, the data typically exceeds simple label associations and tends toward high semantic complexity. 
